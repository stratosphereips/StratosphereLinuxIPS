{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0e9265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from dataset_wrapper import find_and_load_datasets\n",
    "from classifier_wrapper import SKLearnClassifierWrapper\n",
    "from preprocessing_wrapper import PreprocessingWrapper\n",
    "from logger import Logger\n",
    "from features import FeatureExtraction\n",
    "\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "# other models, preprocessing, scalers etc.\n",
    "import warnings\n",
    "\n",
    "# other models, preprocessing, scalers etc.\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning,module=\"sklearn\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71a20e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths, constants\n",
    "root = \"../../../dataset-private/\" # path to the datasets\n",
    "validation_split=0.2\n",
    "experiment_name = \"test_longer_commands\"\n",
    "seed = 1111"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a11bd2b",
   "metadata": {},
   "source": [
    "## Loading, init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fa3055b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found datasets: ['008-zeek-mixed', '009-zeek-malicious', '010-zeek-mixed', '011-zeek-mixed', '012-zeek-mixed', '013-zeek-mixed', '014-zeek-malicious', '015-zeek-malicious']\n",
      "008-zeek-mixed: 5603 samples\n",
      "009-zeek-malicious: 79401 samples\n",
      "010-zeek-mixed: 5310 samples\n",
      "011-zeek-mixed: 8722 samples\n",
      "012-zeek-mixed: 3810 samples\n",
      "013-zeek-mixed: 26241 samples\n",
      "014-zeek-malicious: 26738 samples\n",
      "015-zeek-malicious: 197722 samples\n"
     ]
    }
   ],
   "source": [
    "loaders = find_and_load_datasets(root)\n",
    "\n",
    "found_datasets=list(loaders.keys())\n",
    "print(\"Found datasets:\", found_datasets)\n",
    "\n",
    "\"\"\" results = sample_n_from_each_dataset(loaders,n=3)\n",
    "for ds_name, info in results.items():\n",
    "    print(f\"Dataset: {ds_name}  (file used: {info['file']})  samples: {len(info['samples'])}\")\n",
    "    display(info['df']) \"\"\"\n",
    "\n",
    "for name, loader in loaders.items():\n",
    "    print(f\"{name}: {len(loader)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3b58f1",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "466cc851",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature processing\n",
    "feature_extraction = FeatureExtraction()\n",
    "\n",
    "#preprocessing\n",
    "scaler = StandardScaler() \n",
    "preprocessor = PreprocessingWrapper(experiment_name=experiment_name)\n",
    "preprocessor.add_step(\"scaler\", scaler)\n",
    "pca = IncrementalPCA(n_components=5)\n",
    "preprocessor.add_step(\"pca\", pca)\n",
    "#other steps here?\n",
    "\n",
    "#classifier\n",
    "model = SGDClassifier(loss='hinge', penalty='l1',random_state=seed) \n",
    "classifier = SKLearnClassifierWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adae8344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of commands to run\n",
    "# each command is either \"train\" or \"test\"\n",
    "# dataset_prefix is 3 numbers always - which dataset to use (008, 009, 010, ...)\n",
    "# validation = use validation portion when training\n",
    "commands = [\n",
    "    {\"command\": \"train\", \"dataset_prefix\": \"008\"},#, \"validation\": True},\n",
    "    {\"command\": \"test\", \"dataset_prefix\": \"008\"},\n",
    "    {\"command\": \"test\", \"dataset_prefix\": \"009\"},\n",
    "    {\"command\": \"test\", \"dataset_prefix\": \"015\"},\n",
    "\n",
    "    {\"command\": \"train\", \"dataset_prefix\": \"009\", \"validation\": True},\n",
    "    {\"command\": \"test\", \"dataset_prefix\": \"008\"},\n",
    "    {\"command\": \"test\", \"dataset_prefix\": \"009\"},\n",
    "    {\"command\": \"test\", \"dataset_prefix\": \"017\"}, # skipped, does not exist. if it was first, would not be skipped, exit()\n",
    "\n",
    "    {\"command\": \"train\", \"dataset_prefix\": \"008\"}, \n",
    "    {\"command\": \"test\", \"dataset_prefix\": \"008\"},\n",
    "    {\"command\": \"test\", \"dataset_prefix\": \"009\"},\n",
    "    {\"command\": \"test\", \"dataset_prefix\": \"015\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11037f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is not fitted.\n",
      "Training on dataset 008-zeek-mixed\n",
      "Processing batch 1\n",
      "Testing on dataset 008-zeek-mixed\n",
      "Processing batch 1\n",
      "Testing on dataset 009-zeek-malicious\n",
      "Processing batch 1\n",
      "Processing batch 51\n",
      "Testing on dataset 015-zeek-malicious\n",
      "Processing batch 1\n",
      "Processing batch 51\n",
      "Processing batch 101\n",
      "Processing batch 151\n",
      "Training on dataset 009-zeek-malicious\n",
      "Processing batch 1\n",
      "Processing batch 51\n",
      "Processing batch 101\n",
      "Processing batch 151\n",
      "Testing on dataset 008-zeek-mixed\n",
      "Processing batch 1\n",
      "Testing on dataset 009-zeek-malicious\n",
      "Processing batch 1\n",
      "Processing batch 51\n",
      "No dataset found for 017, skipping\n",
      "Training on dataset 008-zeek-mixed\n",
      "Processing batch 1\n",
      "Testing on dataset 008-zeek-mixed\n",
      "Processing batch 1\n",
      "Testing on dataset 009-zeek-malicious\n",
      "Processing batch 1\n",
      "Processing batch 51\n",
      "Testing on dataset 015-zeek-malicious\n",
      "Processing batch 1\n",
      "Processing batch 51\n",
      "Processing batch 101\n",
      "Processing batch 151\n"
     ]
    }
   ],
   "source": [
    "# datasets we have from before - select one (or a list) to train on!\n",
    "# select commands - train/test on which dataset\n",
    "# train loop\n",
    "    # call batch from dataset\n",
    "    # process features\n",
    "    # preprocessing (scaling)\n",
    "    # train on model with validation, logger for metrics!\n",
    "    # save model after the whole dataset is done\n",
    "    # reporting, metrics, plots, etc.\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Prepare config dictionary\n",
    "\n",
    "# Ensure log experiment folder exists\n",
    "experiment_folder = os.path.join(\".\", \"logs\", experiment_name)\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "\n",
    "# Save config to configs.txt in the experiment folder\n",
    "config_path = os.path.join(experiment_folder, \"configs.txt\")\n",
    "with open(config_path, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"seed\": seed,\n",
    "        \"validation_split\": validation_split,\n",
    "        \"commands\": commands,\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"root\": root\n",
    "    }, f, indent=4)\n",
    "\n",
    "#check if the model is fitted\n",
    "try:\n",
    "    dummy_input = np.zeros((1, model.n_features_in_))\n",
    "    classifier.predict(dummy_input)\n",
    "    is_fitted = True\n",
    "except Exception:\n",
    "    print(\"Model is not fitted.\")\n",
    "    is_fitted = False\n",
    "\n",
    "\n",
    "# main loop doing commands one by one, and storing logs\n",
    "for command_idx,command_dict in enumerate(commands):\n",
    "    #find the dataset we wanted to use\n",
    "    ds = command_dict[\"dataset_prefix\"]\n",
    "    try:    \n",
    "        selected_dataset = next(name for name in found_datasets if ds in name)\n",
    "        loader = loaders[selected_dataset]\n",
    "    except StopIteration:\n",
    "        print(f\"No dataset found for {ds}, skipping\")\n",
    "        if command_idx == 0 and not is_fitted:\n",
    "            print(\"No dataset for the first training command, exiting\")\n",
    "            exit(1)\n",
    "        continue\n",
    "\n",
    "    # based on the command specified, do the action\n",
    "    command = command_dict[\"command\"]\n",
    "    if command == \"train\":\n",
    "        loader.reset_epoch(batch_size=500)\n",
    "        logger = Logger(experiment_name = f\"{experiment_name}\",path_to_logfile = f\"{command_idx}_train_{ds}\", overwrite=True)\n",
    "        print(f\"Training on dataset {selected_dataset}\")\n",
    "        do_validation = command_dict.get(\"validation\", False)\n",
    "\n",
    "        for i in range(loader.batches()):\n",
    "            if i %50 == 0:\n",
    "                print(f\"Processing batch {i}\")\n",
    "            batch = loader.next_batch()\n",
    "            X, y = feature_extraction.process_batch(batch)\n",
    "            sum_labeled_flows = len(y)\n",
    "\n",
    "            if do_validation:\n",
    "                X_train, X_val, y_gt_train, y_gt_val = train_test_split(X, y, test_size=validation_split, random_state=seed)\n",
    "\n",
    "                #preprocessor\n",
    "                preprocessor.partial_fit(X_train)\n",
    "                X_train_processed = preprocessor.transform(X_train)\n",
    "\n",
    "                #classif\n",
    "                classifier.partial_fit(X_train_processed, y_gt_train)\n",
    "                y_pred_train = classifier.predict(X_train_processed)\n",
    "\n",
    "                # predict on validation set\n",
    "                X_val_processed = preprocessor.transform(X_val)\n",
    "                y_pred_val = classifier.predict(X_val_processed)\n",
    "                logger.save_training_results(\n",
    "                    y_pred_train, y_gt_train, y_pred_val, y_gt_val, sum_labeled_flows\n",
    "                )\n",
    "            else:\n",
    "                preprocessor.partial_fit(X)\n",
    "                X_processed = preprocessor.transform(X)\n",
    "                classifier.partial_fit(X_processed, y)\n",
    "                y_pred_train = classifier.predict(X_processed)\n",
    "                logger.save_training_results(\n",
    "                    y_pred_train, y, None, None, sum_labeled_flows # None is for validation\n",
    "                )\n",
    "\n",
    "    elif command == \"test\":\n",
    "        logger = Logger(experiment_name = f\"{experiment_name}\",path_to_logfile = f\"{command_idx}_test_{ds}\", overwrite=True)\n",
    "        loader.reset_epoch(batch_size=1_000)\n",
    "        print(f\"Testing on dataset {selected_dataset}\")\n",
    "        for i in range(loader.batches()):\n",
    "            batch = loader.next_batch()\n",
    "            if i %50 == 0:\n",
    "                print(f\"Processing batch {i}\")\n",
    "            X, y = feature_extraction.process_batch(batch)\n",
    "            if X.shape[0] == 0:\n",
    "                continue\n",
    "            X_processed = preprocessor.transform(X)\n",
    "            y_pred = classifier.predict(X_processed)\n",
    "            logger.save_test_results(y, y_pred)\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(f\"Unknown command {command}, skipping\")\n",
    "        continue\n",
    "\n",
    "#save model, preprocessor steps and add config to the configs.txt\n",
    "classifier.save_classifier(path = f\"./models/{experiment_name}\" ,name = \"model_lin_SGD.bin\")\n",
    "preprocessor.save() #saves in models/<experiment_name>/preprocessing/<step_name>\n",
    "# Append feature names from the first preprocessing step and model parameters to configs.txt\n",
    "# Get feature names from the first step in the preprocessor\n",
    "first_preprocessor_step = preprocessor.steps[0][1]\n",
    "if hasattr(first_preprocessor_step, 'get_feature_names_out'):\n",
    "    feature_names = first_preprocessor_step.get_feature_names_out()\n",
    "elif hasattr(first_preprocessor_step, 'feature_names_in_'):\n",
    "    feature_names = first_preprocessor_step.feature_names_in_\n",
    "else:\n",
    "    feature_names = None\n",
    "\n",
    "# Get model parameters\n",
    "model_params = model.get_params()\n",
    "model_info = {\n",
    "    \"class\": type(model).__name__,\n",
    "    \"loss\": getattr(model, \"loss\", None),\n",
    "    \"params\": model_params\n",
    "}\n",
    "\n",
    "with open(config_path, \"a\") as f:\n",
    "    f.write(\"\\n\\n# Feature names from first preprocessing step:\\n\")\n",
    "    if feature_names is not None:\n",
    "        f.write(json.dumps({\"feature_names\": list(feature_names)}, indent=4))\n",
    "    else:\n",
    "        f.write(\"# Feature names not available\\n\")\n",
    "    f.write(\"\\n\\n# Model information:\\n\")\n",
    "    f.write(json.dumps(model_info, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "800b0b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final test?\n",
    "# on unrelated dataset?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slips_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
