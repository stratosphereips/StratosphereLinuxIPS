{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0e9265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from dataset_wrapper import find_and_load_datasets\n",
    "from classifier_wrapper import SKLearnClassifierWrapper\n",
    "from preprocessing_wrapper import PreprocessingWrapper\n",
    "from logger import Logger\n",
    "from features import FeatureExtraction\n",
    "\n",
    "import subprocess\n",
    "\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# other models, preprocessing, scalers etc.\n",
    "import warnings\n",
    "\n",
    "# other models, preprocessing, scalers etc.\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning,module=\"sklearn\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71a20e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths, constants\n",
    "root = \"../../../dataset-private/\" # path to the datasets\n",
    "validation_split=0.1\n",
    "experiment_name = \"test_longer_commands\"\n",
    "seed = 1111"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a11bd2b",
   "metadata": {},
   "source": [
    "## Loading, init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fa3055b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found datasets: ['008-zeek-mixed', '009-zeek-malicious', '010-zeek-mixed', '011-zeek-mixed', '012-zeek-mixed', '013-zeek-mixed', '014-zeek-malicious', '015-zeek-malicious']\n",
      "008-zeek-mixed: 5603 samples\n",
      "009-zeek-malicious: 79401 samples\n",
      "010-zeek-mixed: 5310 samples\n",
      "011-zeek-mixed: 8722 samples\n",
      "012-zeek-mixed: 3810 samples\n",
      "013-zeek-mixed: 26241 samples\n",
      "014-zeek-malicious: 26738 samples\n",
      "015-zeek-malicious: 197722 samples\n"
     ]
    }
   ],
   "source": [
    "loaders = find_and_load_datasets(root)\n",
    "\n",
    "found_datasets=list(loaders.keys())\n",
    "print(\"Found datasets:\", found_datasets)\n",
    "\n",
    "\"\"\" results = sample_n_from_each_dataset(loaders,n=3)\n",
    "for ds_name, info in results.items():\n",
    "    print(f\"Dataset: {ds_name}  (file used: {info['file']})  samples: {len(info['samples'])}\")\n",
    "    display(info['df']) \"\"\"\n",
    "\n",
    "for name, loader in loaders.items():\n",
    "    print(f\"{name}: {len(loader)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3b58f1",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "466cc851",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature processing\n",
    "feature_extraction = FeatureExtraction()\n",
    "\n",
    "#preprocessing\n",
    "scaler = StandardScaler() \n",
    "preprocessor = PreprocessingWrapper(experiment_name=experiment_name)\n",
    "preprocessor.add_step(\"scaler\", scaler)\n",
    "\"\"\" pca = IncrementalPCA(n_components=5)\n",
    "preprocessor.add_step(\"pca\", pca) \"\"\"\n",
    "#other steps here?\n",
    "\n",
    "#classifier\n",
    "model = SGDClassifier(loss='hinge', penalty='l1',random_state=seed) \n",
    "classifier = SKLearnClassifierWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adae8344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of commands to run\n",
    "# each command is either \"train\" or \"test\"\n",
    "# dataset_prefix is 3 numbers always - which dataset to use (008, 009, 010, ...)\n",
    "# validation = use validation portion when training\n",
    "commands = [\n",
    "    {\"command\": \"train\", \"dataset_prefix\": \"008\"},#, \"validation\": True},\n",
    "    {\"command\": \"test\", \"dataset_prefix\": \"008\"},\n",
    "    {\"command\": \"test\", \"dataset_prefix\": \"009\"},\n",
    "    {\"command\": \"test\", \"dataset_prefix\": \"015\"},\n",
    "    \n",
    "    {\"command\": \"train\", \"dataset_prefix\": \"009\", \"validation\": True}, # evaluation\n",
    "    # training error from training part?\n",
    "    {\"command\": \"test\", \"dataset_prefix\": \"008\"},\n",
    "    {\"command\": \"test\", \"dataset_prefix\": \"009\"},\n",
    "    {\"command\": \"test\", \"dataset_prefix\": \"017\"}, # skipped, does not exist. if it was first, would not be skipped, exit()\n",
    "\n",
    "    {\"command\": \"train\", \"dataset_prefix\": \"008\"}, \n",
    "    {\"command\": \"test\", \"dataset_prefix\": \"008\"},\n",
    "    {\"command\": \"test\", \"dataset_prefix\": \"009\"},\n",
    "    {\"command\": \"test\", \"dataset_prefix\": \"015\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11037f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is not fitted.\n",
      "Training on dataset 008-zeek-mixed\n",
      "Processing batch 0\n",
      "Testing on dataset 008-zeek-mixed\n",
      "Processing batch 0\n",
      "Testing on dataset 009-zeek-malicious\n",
      "Processing batch 0\n",
      "Processing batch 50\n",
      "Testing on dataset 015-zeek-malicious\n",
      "Processing batch 0\n",
      "Processing batch 50\n",
      "Processing batch 100\n",
      "Processing batch 150\n"
     ]
    }
   ],
   "source": [
    "# datasets we have from before - select one (or a list) to train on!\n",
    "# select commands - train/test on which dataset\n",
    "# train loop\n",
    "    # call batch from dataset\n",
    "    # process features\n",
    "    # preprocessing (scaling)\n",
    "    # train on model with validation, logger for metrics!\n",
    "    # save model after the whole dataset is done\n",
    "    # reporting, metrics, plots, etc.\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Ensure log experiment folder exists\n",
    "experiment_folder = os.path.join(\".\", \"logs\", experiment_name)\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "\n",
    "# Save config to configs.txt in the experiment folder\n",
    "config_path = os.path.join(experiment_folder, \"configs.txt\")\n",
    "with open(config_path, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"seed\": seed,\n",
    "        \"validation_split\": validation_split,\n",
    "        \"commands\": commands,\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"root\": root\n",
    "    }, f, indent=4)\n",
    "\n",
    "#check if the model is fitted\n",
    "try:\n",
    "    dummy_input = np.zeros((1, model.n_features_in_))\n",
    "    classifier.predict(dummy_input)\n",
    "    is_fitted = True\n",
    "except Exception:\n",
    "    print(\"Model is not fitted.\")\n",
    "    is_fitted = False\n",
    "\n",
    "\n",
    "# main loop doing commands one by one, and storing logs\n",
    "for command_idx,command_dict in enumerate(commands):\n",
    "    #find the dataset we wanted to use\n",
    "    ds = command_dict[\"dataset_prefix\"]\n",
    "    try:    \n",
    "        selected_dataset = next(name for name in found_datasets if ds in name)\n",
    "        loader = loaders[selected_dataset]\n",
    "    except StopIteration:\n",
    "        print(f\"No dataset found for {ds}, skipping\")\n",
    "        if command_idx == 0 and not is_fitted:\n",
    "            print(\"No dataset for the first training command, exiting\")\n",
    "            exit(1)\n",
    "        continue\n",
    "\n",
    "    # based on the command specified, do the action\n",
    "    command = command_dict[\"command\"]\n",
    "    if command == \"train\":\n",
    "        loader.reset_epoch(batch_size=500)\n",
    "        path_to_logfile = f\"{command_idx}_train_{ds}\"\n",
    "        logger = Logger(experiment_name = f\"{experiment_name}\",path_to_logfile = path_to_logfile, overwrite=True)\n",
    "        print(f\"Training on dataset {selected_dataset}\")\n",
    "        do_validation = command_dict.get(\"validation\", False)\n",
    "\n",
    "        for i in range(loader.batches()):\n",
    "            if i %50 == 0:\n",
    "                print(f\"Processing batch {i}\")\n",
    "            batch = loader.next_batch()\n",
    "            X, y = feature_extraction.process_batch(batch)\n",
    "            sum_labeled_flows = len(y)\n",
    "\n",
    "            if do_validation:\n",
    "                X_train, X_val, y_gt_train, y_gt_val = train_test_split(X, y, test_size=validation_split, random_state=seed)\n",
    "\n",
    "                #preprocessor\n",
    "                preprocessor.partial_fit(X_train)\n",
    "                X_train_processed = preprocessor.transform(X_train)\n",
    "\n",
    "                #classif\n",
    "                classifier.partial_fit(X_train_processed, y_gt_train)\n",
    "                y_pred_train = classifier.predict(X_train_processed)\n",
    "\n",
    "                # predict on validation set\n",
    "                X_val_processed = preprocessor.transform(X_val)\n",
    "                y_pred_val = classifier.predict(X_val_processed)\n",
    "                logger.save_training_results(\n",
    "                    y_pred_train, y_gt_train, y_pred_val, y_gt_val, sum_labeled_flows\n",
    "                )\n",
    "            else:\n",
    "                preprocessor.partial_fit(X)\n",
    "                X_processed = preprocessor.transform(X)\n",
    "                classifier.partial_fit(X_processed, y)\n",
    "                y_pred_train = classifier.predict(X_processed)\n",
    "                logger.save_training_results(\n",
    "                    y_pred_train, y, None, None, sum_labeled_flows # None is for validation\n",
    "                )\n",
    "\n",
    "        # After training, plot the training performance using the external script\n",
    "\n",
    "\n",
    "    elif command == \"test\":\n",
    "        path_to_logfile = f\"{command_idx}_test_{ds}\"\n",
    "        logger = Logger(experiment_name = f\"{experiment_name}\",path_to_logfile = path_to_logfile, overwrite=True)\n",
    "        loader.reset_epoch(batch_size=1_000)\n",
    "        print(f\"Testing on dataset {selected_dataset}\")\n",
    "        for i in range(loader.batches()):\n",
    "            batch = loader.next_batch()\n",
    "            if i %50 == 0:\n",
    "                print(f\"Processing batch {i}\")\n",
    "            X, y = feature_extraction.process_batch(batch)\n",
    "            if X.shape[0] == 0:\n",
    "                continue\n",
    "            X_processed = preprocessor.transform(X)\n",
    "            y_pred = classifier.predict(X_processed)\n",
    "            logger.save_test_results(y, y_pred)\n",
    "        \n",
    "\n",
    "    else:\n",
    "        print(f\"Unknown command {command}, skipping\")\n",
    "        continue\n",
    "\n",
    "#save model, preprocessor steps and add config to the configs.txt\n",
    "classifier.save_classifier(path = f\"./models/{experiment_name}\" ,name = \"model_lin_SGD.bin\")\n",
    "preprocessor.save() #saves in models/<experiment_name>/preprocessing/<step_name>\n",
    "# Append feature names from the first preprocessing step and model parameters to configs.txt\n",
    "# Get feature names from the first step in the preprocessor\n",
    "first_preprocessor_step = preprocessor.steps[0][1]\n",
    "if hasattr(first_preprocessor_step, 'get_feature_names_out'):\n",
    "    feature_names = first_preprocessor_step.get_feature_names_out()\n",
    "elif hasattr(first_preprocessor_step, 'feature_names_in_'):\n",
    "    feature_names = first_preprocessor_step.feature_names_in_\n",
    "else:\n",
    "    feature_names = None\n",
    "\n",
    "# Get model parameters\n",
    "model_params = model.get_params()\n",
    "model_info = {\n",
    "    \"class\": type(model).__name__,\n",
    "    \"loss\": getattr(model, \"loss\", None),\n",
    "    \"params\": model_params\n",
    "}\n",
    "\n",
    "with open(config_path, \"a\") as f:\n",
    "    f.write(\"\\n\\n# Feature names from first preprocessing step:\\n\")\n",
    "    if feature_names is not None:\n",
    "        f.write(json.dumps({\"feature_names\": list(feature_names)}, indent=4))\n",
    "    else:\n",
    "        f.write(\"# Feature names not available\\n\")\n",
    "    f.write(\"\\n\\n# Model information:\\n\")\n",
    "    f.write(json.dumps(model_info, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379b121a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Output folder: /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008\n",
      "[INFO] Reading logfile: logs/test_longer_commands/0_train_008\n",
      "[INFO] Parsed 12 batches from logfile\n",
      "[INFO] Accumulating batch and cumulative metrics...\n",
      "Training plots will be saved to: /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/training\n",
      "[SAVED] /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/training/per_batch/class_counts_batch_training.png\n",
      "[SAVED] /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/training/aggregated/class_counts_aggregated_training.png\n",
      "[INFO] Plotting metrics -> /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/training/per_batch/malware_metrics_batch_training.png\n",
      "[SAVED] /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/training/per_batch/malware_metrics_batch_training.png\n",
      "[SAVED] /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/training/per_batch/malware_metrics_batch_training.png\n",
      "[INFO] Plotting metrics -> /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/training/aggregated/malware_metrics_aggregated_training.png\n",
      "[SAVED] /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/training/aggregated/malware_metrics_aggregated_training.png\n",
      "[SAVED] /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/training/aggregated/malware_metrics_aggregated_training.png\n",
      "[INFO] Plotting accuracy -> /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/training/per_batch/accuracy_batch_training.png\n",
      "[SAVED] /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/training/per_batch/accuracy_batch_training.png\n",
      "[SAVED] /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/training/per_batch/accuracy_batch_training.png\n",
      "[INFO] Plotting accuracy -> /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/training/aggregated/accuracy_aggregated_training.png\n",
      "[SAVED] /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/training/aggregated/accuracy_aggregated_training.png\n",
      "[SAVED] /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/training/aggregated/accuracy_aggregated_training.png\n",
      "[INFO] Plotting metrics -> /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/training/last5/malware_metrics_last5_training.png\n",
      "[SAVED] /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/training/last5/malware_metrics_last5_training.png\n",
      "[SAVED] /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/training/last5/malware_metrics_last5_training.png\n",
      "[INFO] Plotting accuracy -> /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/training/last5/accuracy_last5_training.png\n",
      "[SAVED] /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/training/last5/accuracy_last5_training.png\n",
      "[SAVED] /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/training/last5/accuracy_last5_training.png\n",
      "[SAVED] /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/training/last5/class_counts_last5_training.png\n",
      "[INFO] Plotting metrics -> /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/training/last10/malware_metrics_last10_training.png\n",
      "[SAVED] /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/training/last10/malware_metrics_last10_training.png\n",
      "[SAVED] /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/training/last10/malware_metrics_last10_training.png\n",
      "[INFO] Plotting accuracy -> /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/training/last10/accuracy_last10_training.png\n",
      "[SAVED] /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/training/last10/accuracy_last10_training.png\n",
      "[SAVED] /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/training/last10/accuracy_last10_training.png\n",
      "[SAVED] /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/training/last10/class_counts_last10_training.png\n",
      "[INFO] Not enough batches for last-20 (training), skipping.\n",
      "[SAVED] /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_longer_commands/training/0_train_008/summary.txt\n",
      "\n",
      "=== TRAINING Multi-class (Aggregated) ===\n",
      "Benign-Malicious Acc: 0.7577\n",
      "Malware F1:           0.8384\n",
      "Malware FPR:          0.4839\n",
      "Malware FNR:          0.1617\n",
      "Macro F1:             0.6771\n",
      "Precision:             0.8385\n",
      "Recall:                0.8383\n",
      "\n",
      "=== Per-class metrics (Aggregated) - TRAINING ===\n",
      "Class                 TP       TN       FP       FN      Acc     Prec      Rec       F1\n",
      "Benign               721     3509      677      676   0.7577   0.5157   0.5161   0.5159\n",
      "Malicious           3509      721      676      677   0.7577   0.8385   0.8383   0.8384\n",
      "\n",
      "Summary for Experiment 0_train_008:\n",
      "Total batches processed: 12\n",
      "Data type: Training only\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'subprocess' has no attribute 'stdout'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 24\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cmd[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommand\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     18\u001b[0m     subprocess\u001b[38;5;241m.\u001b[39mrun([\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../plot_train_performance.py\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-f\u001b[39m\u001b[38;5;124m\"\u001b[39m, log_dir,\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-e\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_log,\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--save_folder\u001b[39m\u001b[38;5;124m\"\u001b[39m, save_folder\n\u001b[1;32m     23\u001b[0m     ])\n\u001b[0;32m---> 24\u001b[0m     output_stdout \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m cmd[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommand\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     26\u001b[0m     subprocess\u001b[38;5;241m.\u001b[39mrun([\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../plot_testing_performance.py\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-f\u001b[39m\u001b[38;5;124m\"\u001b[39m, log_dir,\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-e\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_log,\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--save_folder\u001b[39m\u001b[38;5;124m\"\u001b[39m, save_folder\n\u001b[1;32m     31\u001b[0m     ])\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'subprocess' has no attribute 'stdout'"
     ]
    }
   ],
   "source": [
    "# Go through experiment log folders and plot performance for each command\n",
    "for idx, cmd in enumerate(commands):\n",
    "\n",
    "    ds = command_dict[\"dataset_prefix\"]\n",
    "    try:    \n",
    "        selected_dataset = next(name for name in found_datasets if ds in name)\n",
    "        loader = loaders[selected_dataset]\n",
    "    except StopIteration:\n",
    "        print(f\"No dataset found for {ds}, skipping\")\n",
    "        \n",
    "\n",
    "    output_log = f\"{idx}_{cmd['command']}_{cmd['dataset_prefix']}\"\n",
    "    log_dir = os.path.join(\"logs\", experiment_name, output_log)\n",
    "    save_folder = f\"./results/{experiment_name}/\"\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    output_stdout = []\n",
    "    if cmd[\"command\"] == \"train\":\n",
    "        subprocess.run([\n",
    "            \"python\", \"../plot_train_performance.py\",\n",
    "            \"-f\", log_dir,\n",
    "            \"-e\", output_log,\n",
    "            \"--save_folder\", save_folder\n",
    "        ])\n",
    "\n",
    "    elif cmd[\"command\"] == \"test\":\n",
    "        subprocess.run([\n",
    "            \"python\", \"../plot_testing_performance.py\",\n",
    "            \"-f\", log_dir,\n",
    "            \"-e\", output_log,\n",
    "            \"--save_folder\", save_folder\n",
    "        ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800b0b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final test?\n",
    "# on unrelated dataset?\n",
    "# Testing dataset?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slips_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
