{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e9265f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from .dataset_wrapper import find_and_load_datasets\n",
    "from .classifier_wrapper import SKLearnClassifierWrapper\n",
    "from .preprocessing_wrapper import PreprocessingWrapper\n",
    "from .logger import Logger\n",
    "from .features import FeatureExtraction\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning,module=\"sklearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71a20e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths, constants\n",
    "root = \"../../../dataset-private/\" # path to the datasets \n",
    "validation_split=0.1\n",
    "experiment_name = \"test_plotting\"\n",
    "seed = 1111"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a11bd2b",
   "metadata": {},
   "source": [
    "## Loading, init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fa3055b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found datasets: ['001-zeek-scenario-malicious', '003-zeek-scenario-malicious', '008-zeek-mixed', '009-zeek-malicious', '010-zeek-mixed', '011-zeek-mixed', '012-zeek-mixed', '013-zeek-mixed', '014-zeek-malicious', '015-zeek-malicious', '016-zeek-malicious', '017-zeek-malicious', '018-zeek-malicious', '020-zeek-malicious', '021-zeek-malicious', '022-zeek-malicious', '023-zeek-malicious', '024-zeek-malicious', '025-zeek-malicious', '026-zeek-malicious', '027-zeek-malicious', '028-zeek-malicious', '029-zeek-malicious', '030-zeek-malicious', '031-zeek-malicious', '032-zeek-malicious', '033-zeek-malicious', '034-zeek-malicious', '035-zeek-malicious', '036-zeek-malicious', '037-zeek-mixed']\n",
      "001-zeek-scenario-malicious: 253 samples\n",
      "003-zeek-scenario-malicious: 361 samples\n",
      "008-zeek-mixed: 5603 samples\n",
      "009-zeek-malicious: 79401 samples\n",
      "010-zeek-mixed: 5310 samples\n",
      "011-zeek-mixed: 8722 samples\n",
      "012-zeek-mixed: 3810 samples\n",
      "013-zeek-mixed: 26241 samples\n",
      "014-zeek-malicious: 26738 samples\n",
      "015-zeek-malicious: 197722 samples\n",
      "016-zeek-malicious: 34633 samples\n",
      "017-zeek-malicious: 39381 samples\n",
      "018-zeek-malicious: 33986 samples\n",
      "020-zeek-malicious: 35159 samples\n",
      "021-zeek-malicious: 33878 samples\n",
      "022-zeek-malicious: 5533 samples\n",
      "023-zeek-malicious: 5460 samples\n",
      "024-zeek-malicious: 5533 samples\n",
      "025-zeek-malicious: 5341 samples\n",
      "026-zeek-malicious: 5355 samples\n",
      "027-zeek-malicious: 2217 samples\n",
      "028-zeek-malicious: 2136 samples\n",
      "029-zeek-malicious: 2447 samples\n",
      "030-zeek-malicious: 2238 samples\n",
      "031-zeek-malicious: 2196 samples\n",
      "032-zeek-malicious: 26738 samples\n",
      "033-zeek-malicious: 11611 samples\n",
      "034-zeek-malicious: 11679 samples\n",
      "035-zeek-malicious: 11853 samples\n",
      "036-zeek-malicious: 11797 samples\n",
      "037-zeek-mixed: 5850 samples\n"
     ]
    }
   ],
   "source": [
    "loaders = find_and_load_datasets(root) #helper function from dataset_loader.py\n",
    "\n",
    "found_datasets=list(loaders.keys())\n",
    "print(\"Found datasets:\", found_datasets)\n",
    "\n",
    "\"\"\" results = sample_n_from_each_dataset(loaders,n=3)\n",
    "for ds_name, info in results.items():\n",
    "    print(f\"Dataset: {ds_name}  (file used: {info['file']})  samples: {len(info['samples'])}\")\n",
    "    display(info['df']) \"\"\"\n",
    "\n",
    "#print datasets\n",
    "for name, loader in loaders.items():\n",
    "    print(f\"{name}: {len(loader)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3b58f1",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "466cc851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' tree_Hoeff = tree.HoeffdingAdaptiveTreeClassifier(seed=seed,max_size=10,max_depth=15)\\nmodel = ensemble.ADWINBoostingClassifier(seed=seed,model = tree_Hoeff,n_models=5)\\n\\n\\n# model = forest.ARFClassifier(seed=seed,n_models=50, max_size=10, warning_detector=drift.ADWIN(delta=0.05))\\n\\n\\n# final wapper for pipeline\\nclassifier = RiverClassifierWrapper(model,preprocessing_handler=preprocessor)\\n '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feature processing\n",
    "feature_extraction = FeatureExtraction()\n",
    "\n",
    "#preprocessing\n",
    "scaler = StandardScaler() \n",
    "preprocessor = PreprocessingWrapper(experiment_name=experiment_name)\n",
    "preprocessor.add_step(\"scaler\", scaler)\n",
    "\n",
    "\"\"\" pca = IncrementalPCA(n_components=7)\n",
    "preprocessor.add_step(\"pca\", pca) \"\"\"\n",
    "\n",
    "# other steps here? add your own!\n",
    "\n",
    "#classifiers:\n",
    "# sklearn SGD linear\n",
    "model = SGDClassifier(loss='hinge', penalty='l2',random_state=seed) \n",
    "classifier = SKLearnClassifierWrapper(model,preprocessing_handler=preprocessor)\n",
    "\n",
    "\n",
    "\"\"\" tree_Hoeff = tree.HoeffdingAdaptiveTreeClassifier(seed=seed,max_size=10,max_depth=15)\n",
    "model = ensemble.ADWINBoostingClassifier(seed=seed,model = tree_Hoeff,n_models=5)\n",
    "\n",
    "\n",
    "# model = forest.ARFClassifier(seed=seed,n_models=50, max_size=10, warning_detector=drift.ADWIN(delta=0.05))\n",
    "\n",
    "\n",
    "# final wapper for pipeline\n",
    "classifier = RiverClassifierWrapper(model,preprocessing_handler=preprocessor)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adae8344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of commands to run\n",
    "# each command is either \"train\" or \"test\"\n",
    "# dataset_prefix is 3 numbers always - which dataset to use (008, 009, 010, ...)\n",
    "# validation = use validation portion when training\n",
    "\n",
    "commands = [\n",
    "    {\"command\": \"train\", \"dataset_prefix\": \"009\", \"validation\": True},\n",
    "    {\"command\": \"test\", \"dataset_prefix\": \"009\"},\n",
    "\n",
    "]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11037f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is not fitted.\n",
      "Training on dataset 009-zeek-malicious\n",
      "Processing batch 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 5\n",
      "Processing batch 10\n",
      "Processing batch 15\n",
      "Processing batch 20\n",
      "Processing batch 25\n",
      "Processing batch 30\n",
      "Processing batch 35\n",
      "Processing batch 40\n",
      "Processing batch 45\n",
      "Processing batch 50\n",
      "Processing batch 55\n",
      "Processing batch 60\n",
      "Processing batch 65\n",
      "Processing batch 70\n",
      "Processing batch 75\n",
      "Testing on dataset 009-zeek-malicious\n",
      "Processing batch 0\n",
      "Processing batch 25\n",
      "Processing batch 50\n",
      "Processing batch 75\n"
     ]
    }
   ],
   "source": [
    "# train loop\n",
    "    # call batch from dataset\n",
    "    # process features\n",
    "    # preprocessing (scaling)\n",
    "    # train on model with validation, logger for metrics!\n",
    "    # save model after the whole dataset is done\n",
    "    # reporting, metrics, plots, etc.\n",
    "\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "# Ensure log experiment folder exists\n",
    "experiment_folder = os.path.join(\".\", \"logs\", experiment_name)\n",
    "if os.path.exists(experiment_folder):\n",
    "    if os.path.isdir(experiment_folder):\n",
    "        shutil.rmtree(experiment_folder)\n",
    "    else:\n",
    "        os.remove(experiment_folder)\n",
    "os.makedirs(experiment_folder, exist_ok=False)\n",
    "\n",
    "# Save config to configs.txt in the experiment folder (for reproducibility)\n",
    "config_path = os.path.join(experiment_folder, \"configs.txt\")\n",
    "with open(config_path, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"seed\": seed,\n",
    "        \"validation_split\": validation_split,\n",
    "        \"commands\": commands,\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"root\": root\n",
    "    }, f, indent=4)\n",
    "\n",
    "# check if the model is fitted (now just to know if we can test from the start or need to train first)\n",
    "try:\n",
    "    dummy_input = np.zeros((1, model.n_features_in_))\n",
    "    classifier.predict(dummy_input)\n",
    "    is_fitted = True\n",
    "except Exception:\n",
    "    print(\"Model is not fitted.\")\n",
    "    is_fitted = False\n",
    "\n",
    "\n",
    "# main loop doing commands one by one, and storing logs\n",
    "for command_idx,command_dict in enumerate(commands):\n",
    "\n",
    "    #find the dataset we wanted to use\n",
    "    ds = command_dict[\"dataset_prefix\"]\n",
    "    try:    \n",
    "        selected_dataset = next(name for name in found_datasets if ds in name)\n",
    "        loader = loaders[selected_dataset]\n",
    "    except StopIteration:\n",
    "        print(f\"No dataset found for {ds}, skipping\")\n",
    "        if command_idx == 0 and not is_fitted:\n",
    "            print(\"No dataset for the first training command, exiting\")\n",
    "            exit(1)\n",
    "        continue\n",
    "\n",
    "    # based on the command specified, do the action train/test\n",
    "    command = command_dict[\"command\"]\n",
    "    num_batches = command_dict.get(\"batches\", None)\n",
    "    training_batches = loader.batches()\n",
    "    if num_batches is not None:\n",
    "        training_batches = min(num_batches, loader.batches())\n",
    "\n",
    "    if command == \"train\":\n",
    "        loader.reset_epoch(batch_size=500)\n",
    "        path_to_logfile = f\"{command_idx}_train_{ds}\"\n",
    "        logger = Logger(experiment_name = f\"{experiment_name}\", path_to_logfile = path_to_logfile, overwrite=True)\n",
    "        print(f\"Training on dataset {selected_dataset}\")\n",
    "        do_validation = command_dict.get(\"validation\", False)\n",
    "\n",
    "        for i in range(training_batches):\n",
    "            if i %5 == 0:\n",
    "                print(f\"Processing batch {i}\")\n",
    "\n",
    "\n",
    "            batch = loader.next_batch()\n",
    "            X, y = feature_extraction.process_batch(batch)\n",
    "\n",
    "            sum_labeled_flows = len(y)\n",
    "            if do_validation:\n",
    "                try:\n",
    "\n",
    "                    validation_indices = rng.choice(\n",
    "                        X.shape[0],\n",
    "                        size=int(validation_split * X.shape[0]),\n",
    "                        replace=False,\n",
    "                    )\n",
    "                    train_indices = np.array(  # the rest is validation\n",
    "                        sorted(\n",
    "                            set(range(X.shape[0])) - set(validation_indices)\n",
    "                        )\n",
    "                    )\n",
    "                    X_train, X_val, y_gt_train, y_gt_val = X.iloc[train_indices], X.iloc[validation_indices], y.iloc[train_indices], y.iloc[validation_indices]\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during train_test_split: {e}\")\n",
    "                    continue\n",
    "\n",
    "\n",
    "                #preprocessor\n",
    "                preprocessor.partial_fit(X_train)\n",
    "                X_train_processed = preprocessor.transform(X_train)\n",
    "\n",
    "                #classif\n",
    "                classifier.partial_fit(X_train_processed, y_gt_train)\n",
    "                y_pred_train = classifier.predict(X_train_processed)\n",
    "\n",
    "                # predict on validation set\n",
    "                if X_val.shape[0] == 0:\n",
    "                    y_pred_val = np.array([])\n",
    "                else:\n",
    "                    X_val_processed = preprocessor.transform(X_val)\n",
    "                    y_pred_val = classifier.predict(X_val_processed)\n",
    "\n",
    "                logger.save_training_results(\n",
    "                    y_pred_train, y_gt_train, y_pred_val, y_gt_val, sum_labeled_flows\n",
    "                )\n",
    "            else:\n",
    "                preprocessor.partial_fit(X)\n",
    "                X_processed = preprocessor.transform(X)\n",
    "                classifier.partial_fit(X_processed, y)\n",
    "                y_pred_train = classifier.predict(X_processed)\n",
    "                logger.save_training_results(\n",
    "                    y_pred_train, y, None, None, sum_labeled_flows # None is for validation\n",
    "                )\n",
    "\n",
    "        # After training, plot the training performance using the external script, not here!\n",
    "\n",
    "\n",
    "    elif command == \"test\":\n",
    "        path_to_logfile = f\"{command_idx}_test_{ds}\"\n",
    "        logger = Logger(experiment_name = f\"{experiment_name}\",path_to_logfile = path_to_logfile, overwrite=True)\n",
    "        loader.reset_epoch(batch_size=1_000)\n",
    "        print(f\"Testing on dataset {selected_dataset}\")\n",
    "        for i in range(loader.batches()):\n",
    "            batch = loader.next_batch()\n",
    "            if i %25 == 0:\n",
    "                print(f\"Processing batch {i}\")\n",
    "            X, y = feature_extraction.process_batch(batch)\n",
    "            if X.shape[0] == 0:\n",
    "                continue\n",
    "            X_processed = preprocessor.transform(X)\n",
    "            y_pred = classifier.predict(X_processed)\n",
    "            logger.save_test_results(y, y_pred)\n",
    "    else:\n",
    "        print(f\"Unknown command {command}, skipping\")\n",
    "        continue\n",
    "\n",
    "#save model, preprocessor steps and add config to the configs.txt\n",
    "models_path= f\"./models/{experiment_name}\"\n",
    "# ensure models directory is fresh (delete if already created)\n",
    "models_path = f\"./models/{experiment_name}\"\n",
    "if os.path.exists(models_path):\n",
    "    if os.path.isdir(models_path):\n",
    "        shutil.rmtree(models_path)\n",
    "    else:\n",
    "        os.remove(models_path)\n",
    "os.makedirs(models_path, exist_ok=False)\n",
    "classifier.save_classifier(path = models_path ,name = \"model_lin_SGD.bin\")\n",
    "preprocessor.save() #saves in models/<experiment_name>/preprocessing/<step_name>\n",
    "\n",
    "#  append feature names from the first preprocessing step and model parameters to configs.txt\n",
    "# get feature names from the first step in the preprocessor\n",
    "first_preprocessor_step = preprocessor.steps[0][1]\n",
    "if hasattr(first_preprocessor_step, 'get_feature_names_out'):\n",
    "    feature_names = first_preprocessor_step.get_feature_names_out()\n",
    "elif hasattr(first_preprocessor_step, 'feature_names_in_'):\n",
    "    feature_names = first_preprocessor_step.feature_names_in_\n",
    "else:\n",
    "    feature_names = None\n",
    "\n",
    "# Get model parameters\n",
    "if hasattr(model, \"get_params\"):\n",
    "    model_params = model.get_params()\n",
    "else:\n",
    "    model_params = None\n",
    "model_info = {\n",
    "    \"class\": type(model).__name__,\n",
    "    \"loss\": getattr(model, \"loss\", None),\n",
    "    \"params\": model_params\n",
    "}\n",
    "\n",
    "# Append to configs.txt\n",
    "with open(config_path, \"a\") as f:\n",
    "    f.write(\"\\n\\n# Feature names from first preprocessing step:\\n\")\n",
    "    if feature_names is not None:\n",
    "        f.write(json.dumps({\"feature_names\": list(feature_names)}, indent=4))\n",
    "    else:\n",
    "        f.write(\"# Feature names not available\\n\")\n",
    "    f.write(\"\\n\\n# Model information:\\n\")\n",
    "    f.write(json.dumps(model_info, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71e18a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(classifier.classifier, 'n_warnings_detected'):\n",
    "    print(classifier.classifier.n_warnings_detected())\n",
    "\n",
    "if hasattr(classifier.classifier, 'n_drifts_detected'):\n",
    "    print(classifier.classifier.n_drifts_detected())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "379b121a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Reading logfile: logs/test_plotting/0_train_009\n",
      "[INFO] Accumulating batch and cumulative metrics...\n",
      "Validation plots will be saved to: /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_plotting/training/0_train_009/validation\n",
      "Training plots will be saved to: /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_plotting/training/0_train_009/training\n",
      "Comparison plots will be saved to: /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_plotting/training/0_train_009/comparison\n",
      "\n",
      "=== VALIDATION Multi-class (Aggregated) ===\n",
      "Accuracy:             1.0000\n",
      "Malware F1:           1.0000\n",
      "Malware FPR:          0.0000\n",
      "Malware FNR:          0.0000\n",
      "Macro F1:             1.0000\n",
      "Precision:            1.0000\n",
      "Recall:               1.0000\n",
      "MCC:                  1.0000\n",
      "\n",
      "=== TRAINING Multi-class (Aggregated) ===\n",
      "Accuracy:             0.9999\n",
      "Malware F1:           1.0000\n",
      "Malware FPR:          0.0042\n",
      "Malware FNR:          0.0000\n",
      "Macro F1:             0.9989\n",
      "Precision:            0.9999\n",
      "Recall:               1.0000\n",
      "MCC:                  0.9979\n",
      "\n",
      "=== Per-class metrics (Aggregated) - VALIDATION ===\n",
      "Class                 TP       TN       FP       FN      Acc     Prec      Rec       F1\n",
      "Benign                57     3159        0        0   1.0000   1.0000   1.0000   1.0000\n",
      "Malicious           3159       57        0        0   1.0000   1.0000   1.0000   1.0000\n",
      "\n",
      "=== Per-class metrics (Aggregated) - TRAINING ===\n",
      "Class                 TP       TN       FP       FN      Acc     Prec      Rec       F1\n",
      "Benign               474    28830        0        2   0.9999   1.0000   0.9958   0.9979\n",
      "Malicious          28830      474        2        0   0.9999   0.9999   1.0000   1.0000\n",
      "\n",
      "Summary for Experiment 0_train_009:\n",
      "Total batches processed: 80\n",
      "Data type: Training/Validation split\n",
      "[INFO] Output folder: /home/svobojan/StratosphereLinuxIPS/modules/flowmldetection/pipeline_ml_training/results/test_plotting/testing/1_test_009\n",
      "[INFO] Reading testing logfile: logs/test_plotting/1_test_009\n",
      "[INFO] Plotting aggregated class counts (TP+FN per class so-far)...\n",
      "[INFO] Plotting malware metrics (FPR, FNR, F1, Accuracy) over snapshots...\n",
      "[INFO] Saving FPR/FNR-only plot...\n",
      "[INFO] Plotting predicted vs seen counts (per-snapshot) for Malicious & Benign...\n",
      "[INFO] Plotting final confusion matrix (final snapshot)...\n",
      "\n",
      "=== Main final metrics (Aggregated so-far) ===\n",
      "Accuracy:             1.0000\n",
      "Malware F1:           1.0000\n",
      "Malware FPR:          0.0019\n",
      "Malware FNR:          0.0000\n",
      "Macro F1:             0.9995\n",
      "Precision:            1.0000\n",
      "Recall:               1.0000\n",
      "\n",
      "=== Per-class metrics (final snapshot) ===\n",
      "Class                 TP       TN       FP       FN     Prec      Rec       F1\n",
      "Malicious          64094     1033        2        0   1.0000   1.0000   1.0000\n",
      "Benign              1033    64094        0        2   1.0000   0.9981   0.9990\n",
      "\n",
      "Summary for Experiment 1_test_009:\n",
      "Total test lines processed: 80\n"
     ]
    }
   ],
   "source": [
    "# Go through experiment log folders and plot performance for each command\n",
    "\n",
    "# saving place for plots!\n",
    "save_folder = f\"./results/{experiment_name}/\"\n",
    "if os.path.exists(save_folder):\n",
    "    if os.path.isdir(save_folder):\n",
    "        shutil.rmtree(save_folder)\n",
    "    else:\n",
    "        os.remove(save_folder)\n",
    "os.makedirs(save_folder, exist_ok=False)\n",
    "\n",
    "for idx, cmd in enumerate(commands):\n",
    "\n",
    "    ds = command_dict[\"dataset_prefix\"]\n",
    "    try:    \n",
    "        selected_dataset = next(name for name in found_datasets if ds in name)\n",
    "        loader = loaders[selected_dataset]\n",
    "    except StopIteration:\n",
    "        print(f\"No dataset found for {ds}, skipping\")\n",
    "        continue\n",
    "\n",
    "    output_log = f\"{idx}_{cmd['command']}_{cmd['dataset_prefix']}\"\n",
    "    log_dir = os.path.join(\"logs\", experiment_name, output_log)\n",
    "    output_stdout = []\n",
    "    if cmd[\"command\"] == \"train\":\n",
    "        subprocess.run([\n",
    "            \"python\", \"../plot_train_performance.py\",\n",
    "            \"-f\", log_dir,\n",
    "            \"-e\", output_log,\n",
    "            \"--save_folder\", save_folder\n",
    "        ])\n",
    "\n",
    "    elif cmd[\"command\"] == \"test\":\n",
    "        subprocess.run([\n",
    "            \"python\", \"../plot_testing_performance.py\",\n",
    "            \"-f\", log_dir,\n",
    "            \"-e\", output_log,\n",
    "            \"--save_folder\", save_folder\n",
    "        ])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slips-ml-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
